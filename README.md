Didactic is an eval/optimization framework for LLM workflows


1. Anatomy of an LLM execution:

Input:
- System Prompt (system): Task instructions for the LLM, this is the thing we want it to do. High level.
- Context (messages): Information that is passed to the LLM in the `messages` array of the model. This is the context that the LLM uses to fulfill the system prompt.
- Model: The model that is used to fulfill the system prompt.

Output:
- Response: The response from the LLM.


2. Anatomy of an LLM integration:

Input:
- Input: can be anything. For example, an email id (which the workflow will use to fetch the email + attachments from an inbox, perform postprocessing, etc.)
- System Prompt: Task instructions for the LLM, this is the thing we want it to do. High level.
- Context (messages): Generated by the workflow itself at some point during the execution, will be passed to the LLM within the workflow.
- Model: The model that is used to fulfill the system prompt.

Output:
- Reponse: Response from the LLM, possibly processed by the workflow.

The difference between an LLM execution and a LLM workflow is that a LLM execution is a single run of the LLM, while a LLM workflow is an integration workflow that calls an LLM execution internally. We want to be able to evaluate and iterate the system prompt for both of these scenarios. At their core, they both take in inputs and produce an output. The only difference is that an LLM workflow encapsulates the LLM execution and may perform some other actions to arrive at the output. In an LLM integration, the LLM execution may receive `additional context` from the workflow before generating the response. We will normalize these two scenarios into a single concept - **LLM workflow**.


### Didactic
A lightweight TypeScript framework that automates evaluation and system prompt optimization for LLM workflows.

#### How it works

1. You define a config object with your inputs, expected outputs, comparators, executor, and metadata (threshold, max_iterations, max_cost, system_prompt, etc.)
2. You call didactic.eval with your config
3. Didactic runs your executor with each input.
4. Didactic uses comparators to compare the expected outputs with the actual outputs.
5. If optimization is enabled:
  5a. Didactic takes the results of the evaluation, the system prompt, and its own optimization prompt to generate a new system prompt and recursively evaluate it.

#### Concepts
- **inputs**: a list of inputs to your LLM workflow.
- **exepceted_outputs**: list of expected outputs for each input.
- **additional_context**: additional context used to arrive at an expected output.
- **system_prompt**: the system prompt for the target LLM workflow.
- **comparators**: functions to compare expected outputs with actual outputs.
- **executors**: functions that execute an LLM workflow and return the actual outputs. (supports endpoints and functions)
- **mapResponse**: function to map the response from the executor to the expected output shape.
- **threshold** the target success rate for the LLM workflow over all test cases.
- **per test threshold** the target success rate for the LLM workflow over a single test case.
- **maxIerations** the maximum number of iterations to run the LLM prompt optimization loop.
- **maxCost** the maximum cost to run the LLM workflow.


### Comparators

This is the core evaluation mechanism of Didactic. A comparator is just a function that takes in two values and returns: 
  - passed: boolean if the values are considered equal.
  - similarity: number between 0 and 1.0 that indicates how similar the values are. Optional, used for enhanced matching and scoring.

You define comparators in an object:
  ```typescript
  comparators: {
    key: comparatorFunction,
  }
  ```
Didactic will parse the keys of the output and use the respective comparator for that key. For example, consider the output:

```typescript
{
  premium: 12500,
  policyStructure: "claims-made",
  subjectivities: "some subjectivities",
}
```

Your comparators object would look like this:
```typescript
comparators: {
  premium: within({ tolerance: 0.05 }),
  policyStructure: oneOf(["claims-made", "occurrence", "entity"]),
  subjectivities: llmCompare(subjectivitiesComparisonPrompt),
}
```
Didactic will use the `within` comparator for the `premium` key and the `oneOf` comparator for the `policyStructure` key. For unnamed outputs, like:

```typescript
[1, 2, 3, 4]
or
"Some string"
```

You can just pass a single comparator function rather than an object. For example:
```typescript
comparators: exact,
```

Didactic exposes a powerful set of built-in comparators and allows users to easily define their own custom comparators.

### Optimizer

The optimizer is the feedback loop that turns eval results into better prompts.

Optimization loop:
  - Run eval with current system prompt
  - Check if the eval passed the `threshold`
  - If it did, return the latest system propmt
  - If it didn't:
    - For each input/expected output pair, build a prompt with the input, results (field-level diffs), additional context (if any), the current system propmt, and the optimization prompt.
    - Run each prompt through the LLM and collect the patches for the current system prompt. 
    - Use another LLM call to apply the patches to the current system prompt and repeat the optimization loop.
    - If the new system prompt evaluates lower than the previous system prompt, use the previous system prompt in the next iteration.
    - If the new system prompt evaluates higher than the previous system prompt, use the new system prompt in the next iteration.
    - Repeat until `targetAccuracy` is hit or you run out of iterations/budget


#### Usage
```typescript
import { didactic, within, oneOf, name, numeric, date } from "didactic"

const result = await didactic.eval({
  executor: didactic.endpoint("https://api.example.com/extract-quote", {
    headers: { "x-api-key": process.env.API_KEY },
    mapResponse: (res) => res.data.quotes,
    timeout: 60000,
  }),

  comparators: {
    premium: within({ tolerance: 0.05 }),
    policyStructure: oneOf(["claims-made", "occurrence", "entity"]),
    entityName: name,
    effectiveDate: date,
  },

  testCases: [
    {
      input: { emailId: "email-abc123" },
      expected: [{
        premium: 12500,
        policyStructure: "claims-made",
        entityName: "Bay Area Medical Group",
        effectiveDate: "2024-01-15",
      }],
    },
    {
      input: { emailId: "email-def456" },
      expected: [{
        premium: 8200,
        policyStructure: "entity",
        entityName: "Pacific Coast Physicians",
        effectiveDate: "2024-03-01",
      }],
    },
  ],

  perTestThreshold: 0.9,

  optimize: {
    model: "gpt-5-2",
    systemPrompt: "My existing, flawed system prompt",
    targetAccuracy: 0.95,
    maxIterations: 10, // optional
    maxCost: 100, // optional
  }
})

console.log(`${result.passed}/${result.total} passed (${result.accuracy}% field accuracy)`)
```

#### File Structure
src/
    - types.ts // types for the framework
    - comparators.ts // built-in comparators and helpers
    - executors.ts // built-in executors (endpoints and local functions)
    - eval.ts // orchestrate: execute → compare → aggregate
    - matching.ts // hungarian algorithm for array matching
    - optimizer.ts // optimizer loop for the system prompt
    - index.ts // public API
